(11/16/24 | 7:23 PM EST):
Avoid parsing data types within function calls; try to convert one variable to another
data type OUTSIDE the function calls. Also, converting each inserted word to a char 
array outside the function calls substantially decreases time complexity.

(11/16/24 | 11:10 PM EST):
Added a skip list class and some ways to optimize it. I think a little more work needs 
to be done but at this point, I'm pretty satisfied with its current algorithm. I also 
made an adjacent 'SLNode' helper class, which represents a node in a skip list. Their
data types are interchangeable and comparable, so we can decide at any time what kind
of data we can implement with the skip list. I plan on using the skip list to store all
roughly 240,000 vocab words in the 'words.txt' file. This way, accessing any of those
words would be easier and faster to access.

(11/16/24 | 11:19 PM EST):
Been thinking about this all day, but I think we should implement an algorithm for
statistically solving for each guess. Here's how I think it'll work: we take each string
value in the 'words.txt' file and add them to a hash map. The string data will be the key
while its associated value is its place in the dictionary.

(11/17/24 | 12:30 AM EST):
Deleted the skip list class because accessing data using a hash map is a lot faster: I just
realized that a hash map, in this case, wouldn't have collisions with its key-value system;
also, its average time complexity for data retrieval and storage is O(1) whereas the
skip-list's is O(n). Much better to use a hash map instead. All that shit gone out the window
but I feel better now lol